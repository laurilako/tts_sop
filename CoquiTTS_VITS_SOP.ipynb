{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HkOd4jwqIb"
      },
      "source": [
        "**Fine tune or train new VITS model**\n",
        "\n",
        "Script composed from [Coqui TTS](https://https://github.com/coqui-ai/TTS) contributors, the OpenAI team, and (https://https://www.youtube.com/c/ThorstenMueller) (https://https://www.youtube.com/c/NanoNomad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOoSyCc8h4WI"
      },
      "source": [
        "**Run this cell to connect your Google Drive account to save files.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RUb7_iJu1mb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO0B3tVbPAKc"
      },
      "source": [
        "**Set paths and then run the next cell**\n",
        "\n",
        "ds_name is the dataset directory (will be created)\n",
        "\n",
        "output_directory is training storage directory, \n",
        "\n",
        "subdirectory of ds_name (will be created)\n",
        "\n",
        "upload_dir is where your samples are stored (will be created)\n",
        "\n",
        "MODEL_FILE is the default path to the VITS model downloaded using Coqui (do not need to change)\n",
        "\n",
        "RUN_NAME is a short name describing your training run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqNvXLc9ltKb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "ds_name = \"me2-dataset\" #@param {type:\"string\"}\n",
        "output_directory = \"traineroutput\" #@param {type:\"string\"}\n",
        "upload_dir = \"upload\" #@param {type:\"string\"}\n",
        "MODEL_FILE = \"/root/.local/share/tts/tts_models--fi--css10--vits/model_file.pth.tar\" #@param {type:\"string\"}\n",
        "upload_dir = \"/content/drive/MyDrive/\" + upload_dir\n",
        "RUN_NAME = \"VITS-fi\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "OUT_PATH = \"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"\n",
        "!mkdir $upload_dir\n",
        "!mkdir /content/drive/MyDrive/$ds_name\n",
        "!mkdir /content/drive/MyDrive/$ds_name/wavs/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set run type.**\n",
        "\n",
        "Continue to resume an interrupted session\n",
        "\n",
        "restore to begin a new session from the defalt model model file above (download from Coqui Hub using the download cell later on).\n",
        "\n",
        "restore-ckpt is for beginning a new session using a prior fine-tuned checkpoint. You can set this later on in the training section in Part 2."
      ],
      "metadata": {
        "id": "aMFyIkp11IPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_type = \"restore-ckpt\" #@param [\"continue\",\"restore\",\"restore-ckpt\",\"new\"]\n",
        "print(run_type + \" run selected\")"
      ],
      "metadata": {
        "id": "nSrZbKCXxalg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gliaeg0WCT0v"
      },
      "source": [
        "**Download and Build Rnnoise (https://github.com/xiph/rnnoise) and Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RDhGzztyCVg3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install pyloudnorm\n",
        "!git clone https://github.com/xiph/rnnoise.git\n",
        "!sudo apt-get install curl autoconf automake libtool python-dev pkg-config sox ffmpeg\n",
        "%cd /content/rnnoise\n",
        "!sh autogen.sh\n",
        "!sh configure\n",
        "!make clean\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02lFBB2aCedD"
      },
      "source": [
        "**Install Sox, Install OpenAI Whisper STT+Translation (https://github.com/openai/whisper)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "eQYQJlARCgb2"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt install sox\n",
        "!git clone https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/openai/whisper.git \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCXtmr_CmeS"
      },
      "source": [
        "**Install Coqui TTS** (https://github.com/coqui-ai/TTS), espeak-ng phonemeizer (https://github.com/espeak-ng/espeak-ng), download Coqui TTS source and examples from GitHub.\n",
        "**bold text**\n",
        "Currently set to force install Coqui Trainer==0.0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dIh5HKnSCoVP"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt-get install espeak-ng\n",
        "!git clone https://github.com/coqui-ai/TTS.git\n",
        "!pip install TTS\n",
        "!pip install Trainer==0.0.20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional) List pretrained models available on the Coqui Hub**"
      ],
      "metadata": {
        "id": "TZexZefkF1z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tts --list_models"
      ],
      "metadata": {
        "id": "6tLe-D8ucptf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audio Preprocessing Options**\n",
        "\n",
        "Recommended: Leave all 'true'\n",
        "\n",
        "run_denoise use Xiph's rnnoise on samples\n",
        "\n",
        "run_splits split samples based on silence interval of 0.2 seconds and then force a split into 8 second segments. Click view code and find the 'sox' lines to change these intervals.\n",
        "\n",
        "use_audio_filter engage highpass filter 50hz, lowpass fitler 8000hz. Click view code and find the 'sox' lines to change these frequencies if needed.\n",
        "\n",
        "normalize_audio to engage -6db peak -25LUFS normalization"
      ],
      "metadata": {
        "id": "SVmC7zdg4Hbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_denoise = \"True\" #@param [\"True\", \"False\"]\n",
        "run_splits = \"True\" #@param [\"True\", \"False\"]\n",
        "use_audio_filter = \"True\" #@param [\"True\", \"False\"]\n",
        "normalize_audio = \"True\" #@param [\"True\", \"False\"]\n",
        "#start_sil_dur = 0.2 #@param {type:\"number\"}\n",
        "#end_sil_dur = 0.2 #@param {type:\"number\"}\n",
        "#sample_max = 8 #@param {type:\"number\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gVoA_9v34HrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD1Z5nM6D3bY"
      },
      "source": [
        "**Process**\n",
        "This section will convert mp3 and wav files in upload_dir to  22050hz mono wav files.  Then it will pass the wav files through rnnoise.\n",
        "\n",
        "rnnoise output is then segmented based on 0.2 second silences (click show code below, change 0.2 in the sox line to the duration to silence duration if needed)\n",
        "\n",
        "8000hz Highpass and 50hz lowpass filters applied, gain/loudness adjusted to reduce potential clipping, -6db peak normalization and -25db lufs applied.  Should be fine for general purpose.\n",
        "\n",
        "segmented audio is then passed through sox again to force-split any long segments (above 8 seconds) into segments once again.  Files smaller than 35kb are deleted."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "import pyloudnorm as pyln\n",
        "import sys\n",
        "import glob\n",
        "%cd $upload_dir\n",
        "#!ls -al\n",
        "!rm -rf $upload_dir/22k_1ch\n",
        "!mkdir $upload_dir/22k_1ch\n",
        "\n",
        "#Convert and resample uploaded mp3/wav clips to 1 channel, 22khz\n",
        "!find . -name '*.mp3' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 22k_1ch/\"${f%.mp3}\".wav ; done' _ {} +\n",
        "!find . -name '*.wav' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 22k_1ch/\"${f%.wav}\".wav ; done' _ {} +\n",
        "#!ls -al $upload_dir/22k_1ch\n",
        "print(\"Files converted to 22khz 1ch wav\")\n",
        "if run_denoise==\"True\":\n",
        "  print(\"Running denoise...\")\n",
        "  orig_wavs= upload_dir + \"/22k_1ch/\"\n",
        "  print(orig_wavs)\n",
        "\n",
        "  from pathlib import Path\n",
        "  import os\n",
        "  import subprocess\n",
        "  import soundfile as sf\n",
        "  import pyloudnorm as pyln\n",
        "  import sys\n",
        "  import glob\n",
        "  rnn = \"/content/rnnoise/examples/rnnoise_demo\"\n",
        "  paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "  for filepath in paths:\n",
        "    base = os.path.basename(filepath)\n",
        "    tp_s = upload_dir + \"/22k_1ch/denoise/\"\n",
        "    tf_s = upload_dir + \"/22k_1ch/denoise/\" + base\n",
        "    target_path = Path(tp_s)\n",
        "    target_file = Path(tf_s)\n",
        "    print(\"From: \" + str(filepath))\n",
        "    print(\"To: \" + str(target_file))\n",
        "\t\n",
        "  # Stereo to Mono; upsample to 48000Hz\n",
        "  # added -G to fix gain, -v 0.8\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "    subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "    subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "\n",
        "    subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "    if use_audio_filter==\"True\":\n",
        "      print(\"Running highpass/lowpass & resample\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"50\", \"lowpass\", \"8000\", \"rate\", \"22050\"]) \n",
        "      # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "    elif use_audio_filter==\"False\":\n",
        "      print(\"Running resample without filter\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"rate\", \"22050\"]) \n",
        "      # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "# peak normalize audio to -6 dB\n",
        "    if normalize_audio==\"True\":\n",
        "      print(\"Output normalized\")\n",
        "      peak_normalized_audio = pyln.normalize.peak(data, -6.0)\n",
        "\n",
        "# measure the loudness first\n",
        "      meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "      loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "      loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "      sf.write(target_file, data=loudness_normalized_audio, samplerate=22050)\n",
        "      print(\"\")\n",
        "    elif normalize_audio==\"False\":\n",
        "      print(\"File written without normalizing\")\n",
        "      sf.write(target_file, data=data, samplerate=22050)\n",
        "      print(\"\")\n",
        "\n",
        "  !rm $target_path/rnn.wav\n",
        "  !rm $target_path/48k.wav\n",
        "\n",
        "elif run_denoise==\"False\":\n",
        "  paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "  for filepath in paths:\n",
        "    print(\"Skipping denoise...\")\n",
        "    base = os.path.basename(filepath)\n",
        "    tp_s = upload_dir + \"/22k_1ch/denoise/\"\n",
        "    tf_s = upload_dir + \"/22k_1ch/denoise/\" + base\n",
        "    target_path = Path(tp_s)\n",
        "    target_file = Path(tf_s)\n",
        "    print(\"From: \" + str(filepath))\n",
        "    print(\"To: \" + str(target_file))\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "    subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "    #subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "    subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "    if use_audio_filter==\"True\":\n",
        "      print(\"Running filter...\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"50\", \"lowpass\", \"8000\", \"rate\", \"22050\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "    elif use_audio_filter==\"False\":\n",
        "      print(\"Skipping filter...\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"rate\", \"22050\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "          # peak normalize audio to -6 dB\n",
        "    if normalize_audio==\"True\":\n",
        "      print(\"Output normalized\")\n",
        "      peak_normalized_audio = pyln.normalize.peak(data, -6.0)\n",
        "\n",
        "# measure the loudness first\n",
        "      meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "      loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "      loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "      sf.write(target_file, data=loudness_normalized_audio, samplerate=22050)\n",
        "      print(\"\")\n",
        "    if normalize_audio==\"False\":\n",
        "      print(\"File written without normalizing\")\n",
        "      sf.write(target_file, data=data, samplerate=22050)\n",
        "      print(\"\")\n",
        "  !rm $target_path/rnn.wav\n",
        "  !rm $target_path/48k.wav\n",
        "\n",
        "if run_splits==\"False\":\n",
        "  print(\"Copying files without splitting...\")\n",
        "  %mkdir /content/drive/MyDrive/$ds_name\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wavs\n",
        "  !cp $target_path/*.wav /content/drive/MyDrive/$ds_name/wavs\n",
        "if run_splits==\"True\":\n",
        "  print(\"Splitting output and copying...\")\n",
        "  %cd $target_path\n",
        "  !rm -rf splits\n",
        "  !mkdir splits\n",
        "  !for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress silence 1 0.2 0.1% 1 0.2 0.1% : newfile : restart ; done\n",
        "#alt split method: force splits of 8 seconds, however this will split words. Comment the above with # and remove the # below to change\n",
        "#!for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress trim 0 8 : restart ; done\n",
        "  %cd splits\n",
        "  !mkdir resplit\n",
        "  !for FILE in *.wav; do sox \"$FILE\" resplit/\"$FILE\" --show-progress trim 0 8 : newfile : restart ; done\n",
        "  %cd resplit\n",
        "  !find . -name \"*.wav\" -type f -size -35k -delete\n",
        "  #!ls -al\n",
        "  %cd /content/drive/MyDrive/$ds_name/\n",
        "\n",
        "  !mkdir wavs\n",
        "  !cp $target_path/splits/resplit/*.wav /content/drive/MyDrive/$ds_name/wavs\n",
        "  %cd /content/drive/MyDrive/$ds_name/wavs\n",
        "#  !ls -al"
      ],
      "metadata": {
        "id": "jK0-meIDiWzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run this cell once only.**\n",
        "\n",
        "**Load OpenAI Whisper model to memory. Now set to download medium for finnish STT, works pretty well. Loading it multiple times tends to crash Colab.**\n",
        "\n",
        "Click show code and swap the commented line to use the other models instead. read DOCS."
      ],
      "metadata": {
        "id": "-TXAO2xBTBnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import whisper\n",
        "import os, os.path\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "#model = whisper.load_model(\"medium.en\")\n",
        "model = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "LMNKXV9OTAXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Whisper on generated audio clips, generate transcript named metadata.csv in LJSpeech format in the dataset directory.**"
      ],
      "metadata": {
        "id": "bUwJq6WOjNz5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPp7FX6viuQe"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "wavs = '/content/drive/MyDrive/'+ds_name+'/wavs'\n",
        "\n",
        "paths = glob.glob(os.path.join(wavs, '*.wav'))\n",
        "print(len(paths))\n",
        "\n",
        "all_filenames = []\n",
        "transcript_text = []\n",
        "with open('/content/drive/MyDrive/'+ds_name+'/metadata.csv', 'w', encoding='utf-8') as outfile:\n",
        "\tfor filepath in paths:\n",
        "\t\tbase = os.path.basename(filepath)\n",
        "\t\tall_filenames.append(base)\n",
        "\tfor filepath in paths:\n",
        "\t\tresult = model.transcribe(filepath, language=\"fi\")\n",
        "\t\toutput = result[\"text\"].lstrip()\n",
        "\t\toutput = output.replace(\"\\n\",\"\")\n",
        "\t\tthefile = str(os.path.basename(filepath).lstrip(\".\")).rsplit(\".\")[0]\n",
        "\t\toutfile.write(thefile + '|' + output + '|' + output + '\\n')\n",
        "\t\tprint(thefile + '|' + output + '|' + output + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display dataset**"
      ],
      "metadata": {
        "id": "pekKe4Ibeddj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIVhvXY4jb1m",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!cat /content/drive/MyDrive/$ds_name/metadata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX5ftK4TzPUD"
      },
      "source": [
        "**Download VITS model and Generate Sample Wav File to /content/ljspeech-vits.wav  This will be deleted when your Colab session is closed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy-BadvazVNM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!tts --text \"Terve. Olen malli. Minua koulutetaan nyt.\" --model_name \"tts_models/fi/css10/vits\" --out_path /content/ljspeech-vits.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQHeP10Sslx"
      },
      "source": [
        "**Load Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwHjCM2MSuMp"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuIDzc8Svqh"
      },
      "source": [
        "**Load Dashboard**\n",
        "May take several minutes to appear from a blank white box.  Ad blockers probably need to whitelist a bunch of Colab stuff or this won't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIZZele2SxvM"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/$ds_name/$output_directory/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load libs**"
      ],
      "metadata": {
        "id": "g33SanyO17KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig, CharactersConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits, VitsAudioConfig\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n"
      ],
      "metadata": {
        "id": "fLCy2lppSmmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/\"+ds_name + \"/\" + output_directory + \"/\"\n",
        "SKIP_TRAIN_EPOCH = False"
      ],
      "metadata": {
        "id": "tJkyRKAvVmcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", language=\"fi\", path=os.path.join(output_path, \"/content/drive/MyDrive/\"+ ds_name)\n",
        ")"
      ],
      "metadata": {
        "id": "az06kqrkTHHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_config = VitsAudioConfig(\n",
        "    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n",
        ")\n",
        "\n",
        "config = VitsConfig(\n",
        "    audio=audio_config,\n",
        "    run_name=\"vits_ljspeech_oma\",\n",
        "    batch_size=16,\n",
        "    eval_batch_size=16,\n",
        "    batch_group_size=16,\n",
        "#   num_loader_workers=8,\n",
        "    num_loader_workers=2,\n",
        "    num_eval_loader_workers=2,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=1000,\n",
        "    save_step=1000,\n",
        "\t  save_checkpoints=True,\n",
        "\t  save_n_checkpoints=4,\n",
        "\t  save_best_after=1000,\n",
        "    #text_cleaner=\"english_cleaners\",\n",
        "    text_cleaner=\"multilingual_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"fi\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    characters=CharactersConfig(\n",
        "      characters_class=\"TTS.tts.utils.text.characters.Graphemes\",\n",
        "      vocab_dict=None,\n",
        "      pad=\"<PAD>\",\n",
        "      eos=\"<EOS>\",\n",
        "      bos=\"<BOS>\",\n",
        "      blank=\"<BLNK>\",\n",
        "      characters=\"abcdefghijklmnopqrstuvwxyzˈŋˌːøɪɡ\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\",\n",
        "      punctuations=\"!'(),-.:;? \",\n",
        "      phonemes=None,\n",
        "      is_unique=True,\n",
        "      is_sorted=True\n",
        "    ),\n",
        "    compute_input_seq_cache=True,\n",
        "    print_step=25,\n",
        "    print_eval=True,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    cudnn_benchmark=False,\n",
        "    test_sentences=[\n",
        "    [\n",
        "        \"Sateenkaari on spektrin väreissä esiintyvä ilmakehän optinen ilmiö. Se syntyy, kun valo taittuu pisaran etupinnasta, heijastuu pisaran takapinnasta ja taittuu jälleen pisaran etupinnasta.\",\n",
        "        \"css10\",\n",
        "        None,\n",
        "        \"fi\"\n",
        "    ],\n",
        "    [\n",
        "        \"Moi, minun nimeni on Aleksanteri.\",\n",
        "        \"css10\",\n",
        "        None,\n",
        "        \"fi\"\n",
        "    ],\n",
        "    [\n",
        "        \"Tämä on outo ilmiö. En tiedä mitä tässä tapahtuu.\",\n",
        "        \"css10\",\n",
        "        None,\n",
        "        \"fi\"\n",
        "    ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# INITIALIZE THE AUDIO PROCESSOR\n",
        "# Audio processor is used for feature extraction and audio I/O.\n",
        "# It mainly serves to the dataloader and the training loggers.\n",
        "ap = AudioProcessor.init_from_config(config)\n"
      ],
      "metadata": {
        "id": "JxVuLogxTpDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE THE TOKENIZER\n",
        "# Tokenizer is used to convert text to sequences of token IDs.\n",
        "# config is updated with the default characters if not defined in the config.\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# LOAD DATA SAMPLES\n",
        "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
        "# You can define your custom sample loader returning the list of samples.\n",
        "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
        "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")"
      ],
      "metadata": {
        "id": "yPr8sB1FT6Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Vits.init_from_config(config)"
      ],
      "metadata": {
        "id": "EcPMGc2E67Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If continuning a run: use the next cell to list all run directories.**\n",
        "\n",
        "**Copy and paste the run you want to or restore a checkpoint from into the next box**"
      ],
      "metadata": {
        "id": "VGuwEFDGBfAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!ls -al /content/drive/MyDrive/$ds_name/traineroutput"
      ],
      "metadata": {
        "id": "RVGDNfudBfaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run folder to continue from or Run folder that contains your restore checkpoint**"
      ],
      "metadata": {
        "id": "Gri9U9LTBnep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_folder = \"vits_ljspeech_oma-April-12-2023_10+34AM-0000000\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sx4BQgXeBrAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List checkpoints in run folder. The checkpoint only needs to be selected for a restore run.\n",
        "\n",
        "Continuing a run will load the last best loss checkpoint according to the stored config.json in the run directory on its own (a directory is specified for a continue run, and a model file is specified for a restore run)"
      ],
      "metadata": {
        "id": "sqyZslP8w23D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!ls -al /content/drive/MyDrive/$ds_name/traineroutput/$run_folder"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6T9BmZhew5se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If changing to a different \"restore\" checkpoint to begin a new training session with a model you are already training, set the checkpoint filename here**"
      ],
      "metadata": {
        "id": "vk0mupzwxGUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_file = \"checkpoint_105000.pth\" #@param {type:\"string\"}\n",
        "print(ckpt_file + \" selected for restore run\")\n",
        "if run_type==\"continue\":\n",
        "  print(\"Warning:\\n restore checkpoint selected, but run type set to continue.\\nTrainer will load best loss from checkpoint directory.\\n Are you sure this is what you want to do?\\n\\nIf not, change the run type below to 'restore'\")\n",
        "elif run_type==\"restore-ckpt\":\n",
        "  print(\"Warning:\\n restore checkpoint selected, run type set to restore from selected checkpoint, not default base model.\\nIf this is not correct, adjust the run type.\")\n"
      ],
      "metadata": {
        "id": "yif_PE5gxUcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Last chance to change run type**"
      ],
      "metadata": {
        "id": "eQ_rSxIt25na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_type = \"restore-ckpt\" #@param [\"continue\",\"restore\",\"restore-ckpt\",\"new\"]\n",
        "print(run_type + \" run selected\")"
      ],
      "metadata": {
        "id": "qCBRngRn1AfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Session recovery: Reset selected model file back to default predownloaded path)**"
      ],
      "metadata": {
        "id": "pC8-XakMyDNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title /root/.local/share/tts/tts_models--fi--css10--vits/model_file.pth.tar\n",
        "ckpt_file = \"/root/.local/share/tts/tts_models--fi--css10--vits/model_file.pth.tar\"\n",
        "print(ckpt_file + \" selected for restore run\")"
      ],
      "metadata": {
        "id": "ftzF4Rp1yP49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional) Freeze selected modules. Trainer must be reinitilized if these are changed.**"
      ],
      "metadata": {
        "id": "zd_T21URMoXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current reinit_text_encoder value: \" + str(config.model_args.reinit_text_encoder))\n",
        "reinit_te_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if reinit_te_status==\"False\":\n",
        "  print(\"Text encoder will not be reinitilized\")\n",
        "elif reinit_te_status==\"True\":\n",
        "  config.model_args.reinit_text_encoder=True\n",
        "  print(\"Model arguments set to reinitilize text encoder\")\n",
        "  print(\"Current reinit_DP value: \" + str(config.model_args.reinit_DP))\n",
        "reinit_DP_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if reinit_DP_status==\"False\":\n",
        "  print(\"DP will not be reinitilized\")\n",
        "elif reinit_DP_status==\"True\":\n",
        "  config.model_args.reinit_DP=True\n",
        "  print(\"Model arguments set to reinitilize DP\")\n",
        "print(\"Current freeze_waveform_decoder value: \" + str(config.model_args.freeze_waveform_decoder))\n",
        "freeze_waveform_decoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_waveform_decoder_status==\"False\":\n",
        "  print(\"Waveform decoder will NOT be frozen\")\n",
        "  config.model_args.freeze_waveform_decoder=False\n",
        "elif freeze_waveform_decoder_status==\"True\":\n",
        "  config.model_args.freeze_waveform_decoder=True\n",
        "  print(\"Waveform decoder FROZEN\")\n",
        "print(\"Current freeze_flow_decoder value: \" + str(config.model_args.freeze_flow_decoder))\n",
        "freeze_flow_decoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_flow_decoder_status==\"False\":\n",
        "  print(\"Flow decoder will NOT be frozen\")\n",
        "  config.model_args.freeze_flow_decoder=None\n",
        "elif freeze_flow_decoder_status==\"True\":\n",
        "  config.model_args.freeze_flow_decoder=\"True\"\n",
        "  print(\"Flow decoder FROZEN\")\n",
        "print(\"Current freeze_encoder value: \" + str(config.model_args.freeze_encoder))\n",
        "freeze_encoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_encoder_status==\"False\":\n",
        "  print(\"Text encoder will NOT be frozen\")\n",
        "  config.model_args.freeze_encoder=False\n",
        "elif freeze_encoder_status==\"True\":\n",
        "  config.model_args.freeze_encoder=True\n",
        "  print(\"Text encoder FROZEN\")\n",
        "print(\"Current freeze_DP value: \" + str(config.model_args.freeze_DP))\n",
        "freeze_DP_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_DP_status==\"False\":\n",
        "  print(\"Duration predictor will NOT be frozen\")\n",
        "  config.model_args.freeze_DP=False\n",
        "elif freeze_DP_status==\"True\":\n",
        "  config.model_args.freeze_DP=True\n",
        "  print(\"Duration predictor FROZEN\")        "
      ],
      "metadata": {
        "id": "gV4P42IQVyTO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Init the trainer**"
      ],
      "metadata": {
        "id": "SnRe0PQ37hRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print(run_type)\n",
        "if run_type==\"continue\":\n",
        "  CONTINUE_PATH=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder\n",
        "  trainer = Trainer(\n",
        "    TrainerArgs(continue_path=CONTINUE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "    config,\n",
        "    output_path=OUT_PATH,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"restore\":\n",
        "    trainer = Trainer(\n",
        "    TrainerArgs(restore_path=MODEL_FILE, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "    config,\n",
        "    output_path=OUT_PATH,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"restore-ckpt\":\n",
        "  trainer = Trainer(\n",
        "  TrainerArgs(restore_path=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder+\"/\"+ckpt_file, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "  config,\n",
        "  output_path=OUT_PATH,\n",
        "  model=model,\n",
        "  train_samples=train_samples,\n",
        "  eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"new\":\n",
        "  trainer = Trainer(\n",
        "  TrainerArgs(),\n",
        "  config,\n",
        "  output_path=OUT_PATH,\n",
        "  model=model,\n",
        "  train_samples=train_samples,\n",
        "  eval_samples=eval_samples,\n",
        ")"
      ],
      "metadata": {
        "id": "qoqyadnS9GBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run training**"
      ],
      "metadata": {
        "id": "QuElMKd0p1gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit()"
      ],
      "metadata": {
        "id": "_Y5n6uY-7re_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryma4sViztPk"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script to extract model without discriminator."
      ],
      "metadata": {
        "id": "4VTazQyN9ETy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.config import load_config\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "\n",
        "tts_checkpoint=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder+\"/\"+ckpt_file\n",
        "tts_config_path=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder+\"/config.json\"\n",
        "save_checkpoint = \"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder+\"/\"+\"model.pth\"\n",
        "\n",
        "config = load_config(tts_config_path)\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# init model\n",
        "model = Vits(config, ap, tokenizer, speaker_manager=None)\n",
        "model.load_checkpoint(config, tts_checkpoint, eval=True)\n",
        "model.disc = None\n",
        "model_state = model.state_dict()\n",
        "state = {\n",
        "    \"model\": model_state\n",
        "    }\n",
        "torch.save(state, save_checkpoint)"
      ],
      "metadata": {
        "id": "qyMOqSgj8W8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}