{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HkOd4jwqIb"
      },
      "source": [
        "**FINE TUNE OR TRAIN A VITS MODEL WITH COQUI TTS FRAMEWORK USING CUSTOM DATASET**\n",
        "\n",
        "Scripts from https://youtu.be/MU5157dKOHM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO4Uu_LYQFjx"
      },
      "source": [
        "(1) RUN CELL BELOW, THEN RESTART RUNTIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um9bSH53J64n"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install espeak-ng\n",
        "# core deps\n",
        "!pip install numpy==1.21.6\n",
        "!pip install cython==0.29.28\n",
        "!pip install scipy>=1.4.0\n",
        "!pip install torch>=1.7\n",
        "!pip install torchaudio\n",
        "!pip install soundfile\n",
        "!pip install librosa==0.8.0\n",
        "!pip install numba==0.55.1\n",
        "!pip install inflect==5.6.0\n",
        "!pip install tqdm\n",
        "!pip install anyascii\n",
        "!pip install pyyaml\n",
        "!pip install fsspec>=2021.04.0\n",
        "!pip install packaging\n",
        "# deps for examples\n",
        "!pip install flask\n",
        "# deps for inference\n",
        "!pip install pysbd\n",
        "# deps for notebooks\n",
        "!pip install umap-learn==0.5.1\n",
        "!pip install pandas\n",
        "# deps for training\n",
        "!pip install matplotlib\n",
        "# coqui stack\n",
        "!pip install trainer==0.0.20\n",
        "# config management\n",
        "!pip install coqpit>=0.0.16\n",
        "# chinese g2p deps\n",
        "!pip install jieba\n",
        "!pip install pypinyin\n",
        "# japanese g2p deps\n",
        "!pip install mecab-python3==1.0.5\n",
        "!pip install unidic-lite==1.0.8\n",
        "# gruut+supported langs\n",
        "!pip install gruut[de]==2.2.3\n",
        "# deps for korean\n",
        "!pip install jamo\n",
        "!pip install nltk\n",
        "!pip install g2pkk>=0.1.1\n",
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOoSyCc8h4WI"
      },
      "source": [
        "**(2) Run this cell to connect your Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RUb7_iJu1mb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO0B3tVbPAKc"
      },
      "source": [
        "**(3) Set paths and then run the next cell**\n",
        "\n",
        "ds_name is the dataset directory (will be created)\n",
        "\n",
        "output_directory is training storage directory, \n",
        "\n",
        "subdirectory of ds_name (will be created)\n",
        "\n",
        "upload_dir is where your samples are stored (will be created)\n",
        "\n",
        "MODEL_FILE is the default path to the VITS model downloaded using Coqui (do not need to change).\n",
        "Default model paths: /root/.local/share/tts/tts_models--en--ljspeech--vits/model_file.pth or \n",
        "/root/.local/share/tts/tts_models--en--vctk--vits/model_file.pth\n",
        "\n",
        "\n",
        "RUN_NAME is a short name describing your training run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GqNvXLc9ltKb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "ds_name = \"sop\" #@param {type:\"string\"}\n",
        "output_directory = \"traineroutput\" #@param {type:\"string\"}\n",
        "upload_dir = \"upload\" #@param {type:\"string\"}\n",
        "MODEL_FILE = \"/root/.local/share/tts/tts_models--en--ljspeech--vits/model_file.pth\" #@param {type:\"string\"}\n",
        "upload_dir = \"/content/drive/MyDrive/\" + upload_dir\n",
        "RUN_NAME = \"VITS-fi\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "OUT_PATH = \"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"\n",
        "!mkdir $upload_dir\n",
        "!mkdir /content/drive/MyDrive/$ds_name\n",
        "!mkdir /content/drive/MyDrive/$ds_name/wavs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMFyIkp11IPI"
      },
      "source": [
        "**(4) Set run type.**\n",
        "\n",
        "Continue to resume an interrupted session\n",
        "\n",
        "restore to begin a new session from the defalt model model file above (download from Coqui Hub using the download cell later on).\n",
        "\n",
        "restore-ckpt is for beginning a new session using a prior fine-tuned checkpoint. You can set this later on in the training section in Part 2.\n",
        "\n",
        "newmodel is for beginning a new training session with an empty VITS model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSrZbKCXxalg"
      },
      "outputs": [],
      "source": [
        "run_type = \"restore-ckpt\" #@param [\"continue\",\"restore\",\"restore-ckpt\",\"newmodel\"]\n",
        "print(run_type + \" run selected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gliaeg0WCT0v"
      },
      "source": [
        "#5 ONLY ONCE FOR EACH DATASET\n",
        "**Download and Build Rnnoise (https://github.com/xiph/rnnoise) and Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RDhGzztyCVg3"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install pyloudnorm\n",
        "!git clone https://github.com/xiph/rnnoise.git\n",
        "!sudo apt-get install curl autoconf automake libtool python-dev pkg-config sox ffmpeg\n",
        "%cd /content/rnnoise\n",
        "!sh autogen.sh\n",
        "!sh configure\n",
        "!make clean\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02lFBB2aCedD"
      },
      "source": [
        "**Install Sox, Install OpenAI Whisper STT+Translation (https://github.com/openai/whisper)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eQYQJlARCgb2"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt install sox\n",
        "!git clone https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/openai/whisper.git \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCXtmr_CmeS"
      },
      "source": [
        "**Install Coqui TTS** (https://github.com/coqui-ai/TTS), espeak-ng phonemeizer (https://github.com/espeak-ng/espeak-ng), download Coqui TTS source and examples from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "dIh5HKnSCoVP"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt-get install espeak-ng\n",
        "!git clone https://github.com/coqui-ai/TTS.git\n",
        "%cd TTS\n",
        "!pip install -e .[all,dev,notebooks]\n",
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZexZefkF1z-"
      },
      "source": [
        "**(Optional) List pretrained models available on the Coqui Hub**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6tLe-D8ucptf"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!tts --list_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmg-Zd1tgY25"
      },
      "source": [
        "**List folders in sample upload directory**\n",
        "\n",
        "Google Drive can become desyncronized when uploading files with the web interface.  If your sample folder doesn't show up here, wait, or use the desktop application.  If Colab can't see the folder, it can't access the samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRniRXVjtRkV"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd $upload_dir\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHPyAXuN9p6l"
      },
      "source": [
        "**Set the sample uploads subfolder name to process.**\n",
        "\n",
        "Set the name for the new speaker to process.  This will be the speakerid, and stored in the model as VCTK_{name}\n",
        "\n",
        "You can name them both the same thing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOuRO5i2SFJn"
      },
      "source": [
        "**Set the folder to process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZycMAWhR-TD"
      },
      "source": [
        "**List dataset speaker subdirectories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7xei9f-9owi"
      },
      "outputs": [],
      "source": [
        "subfolder = \"finnish2\" #@param {type:\"string\"}\n",
        "newspeakername = \"finnish\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD1Z5nM6D3bY"
      },
      "source": [
        "**Processing Options**\n",
        "\n",
        "This section will convert mp3, ogg, and wav files in upload_dir to  22050hz mono wav files.  Then it will pass the wav files through rnnoise.\n",
        "\n",
        "rnnoise output is then segmented based on 0.2 second silences (click show code below, change 0.2 in the sox line to the duration to silence duration if needed)\n",
        "\n",
        "8000hz Highpass and 50hz lowpass filters applied, gain/loudness adjusted to reduce potential clipping, -6db peak normalization and -25db lufs applied.  Should be fine for general purpose.\n",
        "\n",
        "segmented audio is then passed through sox again to force-split any long segments (above 9 seconds) into segments once again.  Files smaller than 35kb are deleted.\n",
        "\n",
        "files are renamed and converted to .flac format for the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVoA_9v34HrQ"
      },
      "outputs": [],
      "source": [
        "run_denoise = \"True\" #@param [\"True\", \"False\"]\n",
        "run_splits = \"True\" #@param [\"True\", \"False\"]\n",
        "use_audio_filter = \"True\" #@param [\"True\", \"False\"]\n",
        "normalize_audio = \"True\" #@param [\"True\", \"False\"]\n",
        "#start_sil_dur = 0.2 #@param {type:\"number\"}\n",
        "#end_sil_dur = 0.2 #@param {type:\"number\"}\n",
        "#sample_max = 8 #@param {type:\"number\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twffQl5_Ep7q"
      },
      "source": [
        "**Run processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY2weEjij7dA"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "import pyloudnorm as pyln\n",
        "import sys\n",
        "import glob\n",
        "%cd $upload_dir\n",
        "%cd $subfolder\n",
        "#!ls -al\n",
        "!rm -rf $upload_dir/$subfolder/22k_1ch\n",
        "!mkdir $upload_dir/$subfolder/22k_1ch\n",
        "\n",
        "!find . -name '*.mp3' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 22k_1ch/\"${f%.mp3}\".wav ; done' _ {} +\n",
        "!find . -name '*.ogg' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 22k_1ch/\"${f%.ogg}\".wav ; done' _ {} +\n",
        "!find . -name '*.wav' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 22k_1ch/\"${f%.wav}\".wav ; done' _ {} +\n",
        "!ls -al $upload_dir/$subfolder/22k_1ch\n",
        "print(\"Files converted to 22khz 1ch wav\")\n",
        "%cd $upload_dir\n",
        "%cd $subfolder\n",
        "!rm temp.raw\n",
        "!rm rnn.raw\n",
        "%cd 22k_1ch\n",
        "!find . -name \"*.wav\" -type f -size -35k -delete\n",
        "%cd $upload_dir\n",
        "#Convert and resample uploaded mp3/wav clips to 1 channel, 22khz\n",
        "\n",
        "#\n",
        "if run_denoise==\"True\":\n",
        "  print(\"Running denoise...\")\n",
        "  orig_wavs= upload_dir + '/' + subfolder + \"/22k_1ch/\"\n",
        "  print(orig_wavs)\n",
        "\n",
        "  from pathlib import Path\n",
        "  import os\n",
        "  import subprocess\n",
        "  import soundfile as sf\n",
        "  import pyloudnorm as pyln\n",
        "  import sys\n",
        "  import glob\n",
        "  rnn = \"/content/rnnoise/examples/rnnoise_demo\"\n",
        "  paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "  for filepath in paths:\n",
        "    base = os.path.basename(filepath)\n",
        "    tp_s = upload_dir + '/' + subfolder + \"/22k_1ch/denoise/\"\n",
        "    tf_s = upload_dir + '/' + subfolder + \"/22k_1ch/denoise/\" + base\n",
        "    target_path = Path(tp_s)\n",
        "    target_file = Path(tf_s)\n",
        "    print(\"From: \" + str(filepath))\n",
        "    print(\"To: \" + str(target_file))\n",
        "\t\n",
        "\n",
        "  \n",
        "  # Stereo to Mono; upsample to 48000Hz\n",
        "  # added -G to fix gain, -v 0.8\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "    subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "    subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "\n",
        "    subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "    if use_audio_filter==\"True\":\n",
        "      print(\"Running highpass/lowpass & resample\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"50\", \"lowpass\", \"8000\", \"rate\", \"22050\"]) \n",
        "      # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "    elif use_audio_filter==\"False\":\n",
        "      print(\"Running resample without filter\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"rate\", \"22050\"]) \n",
        "      # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "# peak normalize audio to -6 dB\n",
        "    if normalize_audio==\"True\":\n",
        "      print(\"Output normalized\")\n",
        "      peak_normalized_audio = pyln.normalize.peak(data, -6.0)\n",
        "\n",
        "# measure the loudness first\n",
        "      meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "      loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "      loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "      sf.write(target_file, data=loudness_normalized_audio, samplerate=22050)\n",
        "      print(\"\")\n",
        "    elif normalize_audio==\"False\":\n",
        "      print(\"File written without normalizing\")\n",
        "      sf.write(target_file, data=data, samplerate=22050)\n",
        "      print(\"\")\n",
        "\n",
        "  !rm $target_path/rnn.wav\n",
        "  !rm $target_path/48k.wav\n",
        "\n",
        "elif run_denoise==\"False\":\n",
        "  paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "  for filepath in paths:\n",
        "    print(\"Skipping denoise...\")\n",
        "    base = os.path.basename(filepath)\n",
        "    tp_s = upload_dir + '/' + subfolder + \"/22k_1ch/denoise/\"\n",
        "    tf_s = upload_dir + '/' + subfolder + \"/22k_1ch/denoise/\" + base\n",
        "    target_path = Path(tp_s)\n",
        "    target_file = Path(tf_s)\n",
        "    print(\"From: \" + str(filepath))\n",
        "    print(\"To: \" + str(target_file))\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "    subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "    #subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "    subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "    if use_audio_filter==\"True\":\n",
        "      print(\"Running filter...\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"50\", \"lowpass\", \"8000\", \"rate\", \"22050\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "    elif use_audio_filter==\"False\":\n",
        "      print(\"Skipping filter...\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"rate\", \"22050\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "          # peak normalize audio to -6 dB\n",
        "    if normalize_audio==\"True\":\n",
        "      print(\"Output normalized\")\n",
        "      peak_normalized_audio = pyln.normalize.peak(data, -6.0)\n",
        "\n",
        "# measure the loudness first\n",
        "      meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "      loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "      loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "      sf.write(target_file, data=loudness_normalized_audio, samplerate=22050)\n",
        "      print(\"\")\n",
        "    if normalize_audio==\"False\":\n",
        "      print(\"File written without normalizing\")\n",
        "      sf.write(target_file, data=data, samplerate=22050)\n",
        "      print(\"\")\n",
        "  !rm $target_path/rnn.wav\n",
        "  !rm $target_path/48k.wav\n",
        "\n",
        "if run_splits==\"False\":\n",
        "  print(\"Copying files without splitting...\")\n",
        "  %mkdir /content/drive/MyDrive/$ds_name\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "\n",
        "  !cp $target_path/*.wav /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "if run_splits==\"True\":\n",
        "  %mkdir /content/drive/MyDrive/$ds_name\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername  \n",
        "  print(\"Splitting output and copying...\")\n",
        "  %cd $target_path\n",
        "  !rm -rf splits\n",
        "  !mkdir splits\n",
        "  !for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress silence 1 0.2 0.1% 1 0.2 0.1% : newfile : restart ; done\n",
        "#alt split method: force splits of 9.5 seconds, however this will split words. Comment the above with # and remove the # below to change\n",
        "#!for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress trim 0 8 : restart ; done\n",
        "  %cd splits\n",
        "  !mkdir resplit\n",
        "  !for FILE in *.wav; do sox \"$FILE\" resplit/\"$FILE\" --show-progress trim 0 9 : newfile : restart ; done\n",
        "  %cd resplit\n",
        "  !find . -name \"*.wav\" -type f -size -35k -delete\n",
        "  #!ls -al\n",
        "  %cd /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "\n",
        "#  !ls -al\n",
        "!cp $target_path/splits/resplit/*.wav /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "%cd /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "!rm *.flac\n",
        "!find . -name '*.wav' -exec bash -c 'for f; do ffmpeg -i \"$f\" -c:a flac \"${f%.wav}\"_mic1.flac ; done' _ {} +\n",
        "!rm *.wav\n",
        "!ls /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1oXPWSAYkFv"
      },
      "source": [
        "**Run Whisper on generated audio clips.**\n",
        "Transcripts will be formatted for the VCTK-style dataset and placed in the<br>\n",
        "->dataset directory<br>\n",
        "---->txt<br>\n",
        "-------->newspeakername<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TXAO2xBTBnF"
      },
      "source": [
        "# 6 ONLY ONCE FOR EACH DATASET\n",
        "**Select Whisper STT model. Large-v2 slowest, most accurate, most memory usage. Free Colab users may need to use medium.en.  Select model, run next cell to load it.**\n",
        "\n",
        "**Run the load cell only once. Reloading Whisper STT models may crash your Colab session.**\n",
        "\n",
        "-> Changed model to medium, language to finnish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azm9pXL-9Xuu"
      },
      "outputs": [],
      "source": [
        "whisper_model = \"medium\" #@param [\"large-v2\", \"large-v1\", \"medium\", \"small.en\", \"base.en\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMNKXV9OTAXy"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import whisper\n",
        "import os, os.path\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "#model = whisper.load_model(\"medium.en\")\n",
        "model = whisper.load_model(whisper_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPDgqfwL0VYe"
      },
      "source": [
        "List speakers in dataset directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psssJnAtRp4-"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!ls /content/drive/MyDrive/$ds_name/wav48_silence_trimmed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bOjwfm50YTP"
      },
      "source": [
        "New speaker to process for transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MucYQeY8RlcC"
      },
      "outputs": [],
      "source": [
        "newspeakername = \"finnish\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_UwyvbWJOs"
      },
      "source": [
        "**Run this cell to transcribe clips using Whisper.**\n",
        "\n",
        "To process additional speakers, set a new subfolder/newspeakername above, and remember to re-run that cell before running the one below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApmJnqzAP6Ee"
      },
      "source": [
        "Whisper transcription language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxWY1vZQZ96"
      },
      "outputs": [],
      "source": [
        "whisper_lang = \"finnish\" #@param {type:\"string\"}\n",
        "whisper_lang = whisper_lang.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPp7FX6viuQe"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "options = dict(language=whisper_lang, beam_size=5, best_of=5)\n",
        "transcribe_options = dict(task=\"transcribe\", **options)\n",
        "\n",
        "#@title\n",
        "wavs = '/content/drive/MyDrive/'+ds_name+'/wav48_silence_trimmed/'+newspeakername\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "if os.path.exists(modelpath):\n",
        "\tif os.path.isfile(modelpath+\"large-v2.pt\"):\n",
        "\t\tprint(\"Saved large-v2 found\")\n",
        "\t\tmodel = whisper.load_model(download_root=modelpath,name=\"large-v2\")\n",
        "else:\n",
        "\tprint(\"loading model from Huggingface\")\n",
        "\tmodel = whisper.load_model(\"large-v2\")\n",
        "\"\"\"\n",
        "paths = glob.glob(os.path.join(wavs, '*.flac'))\n",
        "print(len(paths))\n",
        "all_filenames = []\n",
        "transcript_text = []\n",
        "try: \n",
        "\tos.mkdir('/content/drive/MyDrive/'+ds_name+'/txt/') \n",
        "except OSError as error: \n",
        "\tprint(error)  \n",
        "try: \n",
        "\tos.mkdir('/content/drive/MyDrive/'+ds_name+'/txt/'+newspeakername+'/') \n",
        "except OSError as error: \n",
        "\tprint(error)  \n",
        "\n",
        "for filepath in paths:\n",
        "\tbase = os.path.basename(filepath)\n",
        "\tall_filenames.append(base)\n",
        "\tresult = model.transcribe(filepath)\n",
        "\toutput = result[\"text\"].lstrip()\n",
        "\toutput = output.replace(\"\\n\",\"\")\n",
        "\tprint(output)\n",
        "\tthefile = str(os.path.basename(filepath).lstrip(\".\")).rsplit(\".\")[0]\n",
        "\tthefile = thefile[:-5]\n",
        "\tprint(thefile)\n",
        "\toutfilepath = '/content/drive/MyDrive/'+ds_name+'/txt/'+newspeakername+'/'+thefile+'.txt'\n",
        "\twith open(outfilepath, 'w', encoding='utf-8') as indfile:\n",
        "\t\tindfile.write(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYtaWUsHej4A"
      },
      "source": [
        "Perform the steps above for each new speaker.  You should have a folder arraged like this for your dataset.<br>\n",
        "datasetfolder<br>\n",
        "->txt<br>\n",
        "---->nameone<br>\n",
        "---->nametwo<br>\n",
        "->wav48_silence_trimmed<br>\n",
        "---->nameone<br>\n",
        "---->nametwo<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_LNuHJ3M4QJ"
      },
      "source": [
        "**Check dataset for empty transcript files. Set dataset name and speaker. Process each speaker individually.  Bad file sets will be moved to a folder called badfiles in your dataset directory.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIYU5aqVRQzX"
      },
      "outputs": [],
      "source": [
        "ds_name2 = \"sop\" #@param {type:\"string\"}\n",
        "newspeakername2 = \"finnish2\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8eKyvq2dtoM"
      },
      "source": [
        "Scan files, move broken pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebfHT0JIM4oJ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import os, os.path\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "flac = '/content/drive/MyDrive/'+ds_name2+'/wav48_silence_trimmed/'+newspeakername2+'/'\n",
        "wav = '/content/drive/MyDrive/'+ds_name2+'/wavs/'\n",
        "txt = '/content/drive/MyDrive/'+ds_name2+'/txt/'+newspeakername2+'/'\n",
        "outfilepath = '/content/drive/MyDrive/'+ds_name2+'/txt/'+newspeakername2\n",
        "backup_path = '/content/drive/MyDrive/'+ds_name2+'/badfiles/'\n",
        "if not os.path.exists(backup_path):\n",
        "    os.makedirs(backup_path)\n",
        "searchstr = \"(?i)(?!(?![×Þß÷þø])[a-zÀ-ÿ])\\S\"\n",
        "txtfiles = glob.glob(os.path.join(txt, '*.txt'))\n",
        "EMPTY_TRANSCRIPTS = []\n",
        "for txts in txtfiles:\n",
        "    with open(txts, 'r', encoding='utf-8') as outfile:\n",
        "\n",
        "      state = os.stat(txts).st_size == 0\n",
        "      if state==True:\n",
        "          print(str(txts))\n",
        "          basename_without_ext = os.path.splitext(os.path.basename(txts))[0]\n",
        "          print(basename_without_ext)\n",
        "          bad_wav = basename_without_ext+\".wav\"\n",
        "          bad_txt = basename_without_ext+\".txt\"\n",
        "          bad_flac = basename_without_ext+\"_mic1.flac\"\n",
        "          print(bad_wav)\n",
        "          print(bad_txt)\n",
        "          bad_wav_file_size = os.path.getsize(wav+bad_wav)\n",
        "          bad_txt_file_size = os.path.getsize(txt+bad_txt)\n",
        "          bad_flac_file_size = os.path.getsize(flac+bad_flac)\n",
        "          print(bad_wav_file_size)\n",
        "          print(bad_txt_file_size)\n",
        "          shutil.move(wav+bad_wav, backup_path+bad_wav)\n",
        "          shutil.move(txt+bad_txt, backup_path+bad_txt)\n",
        "          shutil.move(flac+bad_flac, backup_path+bad_flac)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS2zF2Cxuvnv"
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "**Part 2 - Training**\n",
        "\n",
        "(1) Download TTS model\n",
        "\n",
        "(2) Load Tensorboard and dashboard\n",
        "\n",
        "(3) Set training variables, load trainer, train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX5ftK4TzPUD"
      },
      "source": [
        "**OPTIONAL, CURRENTLY NOT USED SINCE TRAINING NEW MODEL INSTEAD OF FINE TUNING EXISTING ONE**\n",
        "**Download VITS model and Generate Sample Wav File to /content/ljspeech-vits.wav  This will be deleted when your Colab session is closed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iAwN1tHnX2mw"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!tts --text \"Olen suomalainen malli ja nyt minua jatkokoulutetaan.\" --model_name \"tts_models/fi/css10/vits\" --out_path /content/ljspeech-vits.wav\n",
        "#!tts --text \"I am the very model of a modern Major General\" --model_name \"tts_models/en/vctk/vits\" --out_path /content/ljspeech-vits.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQHeP10Sslx"
      },
      "source": [
        "**Load Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwHjCM2MSuMp"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuIDzc8Svqh"
      },
      "source": [
        "**Load Dashboard**\n",
        "May take several minutes to appear from a blank white box.  Ad blockers probably need to whitelist a bunch of Colab stuff or this won't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIZZele2SxvM"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/$ds_name/$output_directory/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGuwEFDGBfAR"
      },
      "source": [
        "**If continuning a run: use the next cell to list all run directories.**\n",
        "\n",
        "**Copy and paste the run you want to or restore a checkpoint from into the next box**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVGDNfudBfaY"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!ls -al /content/drive/MyDrive/$ds_name/traineroutput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gri9U9LTBnep"
      },
      "source": [
        "**Run folder to continue from or Run folder that contains your restore checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sx4BQgXeBrAk"
      },
      "outputs": [],
      "source": [
        "run_folder = \"sop-March-05-2023_11+22AM-0000000\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqyZslP8w23D"
      },
      "source": [
        "List checkpoints in run folder. The checkpoint only needs to be selected for a restore run.\n",
        "\n",
        "Continuing a run will load the last best loss checkpoint according to the stored config.json in the run directory on its own (a directory is specified for a continue run, and a model file is specified for a restore run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6T9BmZhew5se"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!ls -al /content/drive/MyDrive/$ds_name/traineroutput/$run_folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk0mupzwxGUw"
      },
      "source": [
        "**If changing to a different \"restore\" checkpoint to begin a new training session with a model you are already training, set the checkpoint filename here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yif_PE5gxUcC"
      },
      "outputs": [],
      "source": [
        "ckpt_file = \"best_model_7569.pth\" #@param {type:\"string\"}\n",
        "print(ckpt_file + \" selected for restore run\")\n",
        "if run_type==\"continue\":\n",
        "  print(\"Warning:\\n restore checkpoint selected, but run type set to continue.\\nTrainer will load best loss from checkpoint directory.\\n Are you sure this is what you want to do?\\n\\nIf not, change the run type below to 'restore'\")\n",
        "elif run_type==\"restore-ckpt\":\n",
        "  print(\"Warning:\\n restore checkpoint selected, run type set to restore from selected checkpoint, not default base model.\\nIf this is not correct, adjust the run type.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ_rSxIt25na"
      },
      "source": [
        "**Last chance to change run type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCBRngRn1AfH"
      },
      "outputs": [],
      "source": [
        "run_type = \"restore-ckpt\" #@param [\"continue\",\"restore\",\"restore-ckpt\",\"newmodel\"]\n",
        "print(run_type + \" run selected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1AI-YzRnGsW"
      },
      "source": [
        "**Run the next cells in order to begin training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrcQXOi8aPor"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig, CharactersConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits, VitsArgs, VitsAudioConfig\n",
        "from TTS.tts.utils.speakers import SpeakerManager\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.bin.compute_embeddings import compute_embeddings\n",
        "from TTS.tts.utils.data import get_length_balancer_weights\n",
        "from TTS.tts.utils.languages import LanguageManager, get_language_balancer_weights\n",
        "from TTS.tts.utils.speakers import SpeakerManager, get_speaker_balancer_weights, get_speaker_manager\n",
        "from TTS.tts.utils.text.characters import Graphemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbhLXWPwaSBr"
      },
      "outputs": [],
      "source": [
        "  #output_path = os.path.dirname(os.path.abspath(__file__))\n",
        "output_path = os.path.dirname(\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\")\n",
        "print(output_path)\n",
        "SKIP_TRAIN_EPOCH=False\n",
        "#https://github.com/coqui-ai/TTS/releases/tag/speaker_encoder_model\n",
        "## Extract speaker embeddings\n",
        "\n",
        "SPEAKER_ENCODER_CHECKPOINT_PATH = (\n",
        "    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n",
        ")\n",
        "\n",
        "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
        "SKIP_TRAIN_EPOCH = False\n",
        "BATCH_SIZE = 16\n",
        "SAMPLE_RATE = 22050\n",
        "MAX_AUDIO_LEN_IN_SECONDS = 10\n",
        "NUM_RESAMPLE_THREADS = 10\n",
        "\n",
        "\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"vctk\", meta_file_train=\"\", phonemizer=\"espeak\", dataset_name=\"sop\", language=\"fi\", path=\"/content/drive/MyDrive/\"+ds_name\n",
        ")\n",
        "\n",
        "print(dataset_config)\n",
        "DATASETS_CONFIG_LIST = [dataset_config]\n",
        "D_VECTOR_FILES=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elKFrJg9URQZ"
      },
      "outputs": [],
      "source": [
        "audio_config = VitsAudioConfig(\n",
        "    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n",
        ")\n",
        "\n",
        "vitsArgs = VitsArgs(\n",
        "    use_d_vector_file=True,\n",
        "    d_vector_file=D_VECTOR_FILES,\n",
        "    d_vector_dim=512,\n",
        "    num_layers_text_encoder=6,\n",
        "    embedded_language_dim=4,\n",
        "    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
        "    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n",
        "    use_language_embedding=False,\n",
        "    use_speaker_embedding=False,\n",
        "    use_speaker_encoder_as_loss=True,\n",
        "    use_sdp=True,\n",
        ")\n",
        "\n",
        "config = VitsConfig(\n",
        "    model_args=vitsArgs,\n",
        "    audio=audio_config,\n",
        "    run_name=\"sop\",\n",
        "    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n",
        "    min_text_len=1,\n",
        "    min_audio_len=1,\n",
        "    #max_text_len=325,\n",
        "    batch_size=12,\n",
        "    eval_batch_size=4,\n",
        "    batch_group_size=1,\n",
        "    num_loader_workers=2,\n",
        "    num_eval_loader_workers=2,\n",
        "    eval_split_max_size=256,\n",
        "    #eval_split_size=0.014084507042253521,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=10000,\n",
        "    save_step=1000,\n",
        "    save_checkpoints=True,\n",
        "    save_best_after=1000,\n",
        "    save_n_checkpoints=4,\n",
        "    #use_weighted_sampler=True,\n",
        "    use_weighted_sampler=False,\n",
        "    start_by_longest=True,\n",
        "    weighted_sampler_attrs={\"speaker_name\": 1.0},\n",
        "    weighted_sampler_multipliers={\"speaker_name\": {}},    \n",
        "    speaker_encoder_loss_alpha=9.0,\n",
        "    text_cleaner=\"multilingual_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"fi\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    characters=CharactersConfig(\n",
        "      characters_class=\"TTS.tts.utils.text.characters.Graphemes\",\n",
        "      vocab_dict=None,\n",
        "      pad=\"<PAD>\",\n",
        "      eos=\"<EOS>\",\n",
        "      bos=\"<BOS>\",\n",
        "      blank=\"<BLNK>\",\n",
        "      characters=\"abcdefghijklmnopqrstuvwxyz\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\",\n",
        "      punctuations=\"!'(),-.:;? \",\n",
        "      #phonemes=None,\n",
        "      is_unique=True,\n",
        "      is_sorted=True\n",
        "    ),\n",
        "    compute_input_seq_cache=True,\n",
        "    print_step=50,\n",
        "    print_eval=True,\n",
        "    mixed_precision=False,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    cudnn_benchmark=False,\n",
        "    test_sentences=[\n",
        "        [\n",
        "            \"Sateenkaari on spektrin v\\u00e4reiss\\u00e4 esiintyv\\u00e4 ilmakeh\\u00e4n optinen ilmi\\u00f6. Se syntyy, kun valo taittuu pisaran etupinnasta, heijastuu pisaran takapinnasta ja taittuu j\\u00e4lleen pisaran etupinnasta.\",\n",
        "            #\"css10\",\n",
        "            None,\n",
        "            \"fi\",\n",
        "        ],\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vrrycOY-CAo"
      },
      "outputs": [],
      "source": [
        "# INITIALIZE THE AUDIO PROCESSOR\n",
        "# Audio processor is used for feature extraction and audio I/O.\n",
        "# It mainly serves to the dataloader and the training loggers.\n",
        "ap = AudioProcessor.init_from_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yY6I4Xmculj"
      },
      "outputs": [],
      "source": [
        "for dataset_conf in DATASETS_CONFIG_LIST:\n",
        "    # Check if the embeddings weren't already computed, if not compute it\n",
        "    print(dataset_conf.path)\n",
        "    embbase=str(dataset_conf.dataset_name)\n",
        "    #embeddings_file = MODEL_DIR+\"speakers.pth\"\n",
        "    embeddings_file = os.path.join(dataset_conf.path, embbase+\"_speakers.pth\")\n",
        "    print(embeddings_file)\n",
        "    if not os.path.isfile(embeddings_file):\n",
        "        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n",
        "        print(SPEAKER_ENCODER_CHECKPOINT_PATH)\n",
        "        print(SPEAKER_ENCODER_CONFIG_PATH)\n",
        "        print(embeddings_file)\n",
        "        print(dataset_conf.formatter)\n",
        "        print(dataset_conf.dataset_name)\n",
        "        print(dataset_conf.path)\n",
        "        print(dataset_conf.meta_file_train)\n",
        "        print(dataset_conf.meta_file_val)\n",
        "        compute_embeddings(\n",
        "            SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
        "            SPEAKER_ENCODER_CONFIG_PATH,\n",
        "            embeddings_file,\n",
        "            old_spakers_file=None,\n",
        "            config_dataset_path=None,\n",
        "            formatter_name=dataset_conf.formatter,\n",
        "            dataset_name=dataset_conf.dataset_name,\n",
        "            dataset_path=dataset_conf.path,\n",
        "            meta_file_train=dataset_conf.meta_file_train,\n",
        "            meta_file_val=dataset_conf.meta_file_val,\n",
        "            disable_cuda=False,\n",
        "            no_eval=False,\n",
        "        )\n",
        "    D_VECTOR_FILES.append(embeddings_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FRMmrAYexdR"
      },
      "outputs": [],
      "source": [
        "speaker_manager = SpeakerManager(\n",
        "    d_vectors_file_path=D_VECTOR_FILES,\n",
        "    encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
        "    encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxnMZ07RUl_5"
      },
      "outputs": [],
      "source": [
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8XODrVFWGne"
      },
      "outputs": [],
      "source": [
        "model = Vits(config, ap, tokenizer, speaker_manager)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QF4g7F2VAqZ"
      },
      "outputs": [],
      "source": [
        "train_samples, eval_samples = load_tts_samples(\n",
        "    DATASETS_CONFIG_LIST,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq-XF5zPMHOc"
      },
      "source": [
        "**Display complete character set from all datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr4GHM26Ek5N"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import os\n",
        "import re\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits, VitsArgs, VitsAudioConfig\n",
        "from TTS.tts.utils.speakers import SpeakerManager\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.bin.compute_embeddings import compute_embeddings\n",
        "from TTS.tts.utils.data import get_length_balancer_weights\n",
        "from TTS.tts.utils.languages import LanguageManager, get_language_balancer_weights\n",
        "from TTS.tts.utils.speakers import SpeakerManager, get_speaker_balancer_weights, get_speaker_manager\n",
        "\n",
        "from tqdm.contrib.concurrent import process_map\n",
        "\n",
        "from TTS.config import load_config\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.utils.text.phonemizers import espeak_wrapper\n",
        "from TTS.tts.utils.text.phonemizers import ESpeak\n",
        "import multiprocessing\n",
        "\n",
        "def compute_phonemes(item):\n",
        "    text = item[\"text\"]\n",
        "    ph = phonemizer.phonemize(text).replace(\"|\", \"\")\n",
        "    return set(list(ph))\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    DATASETS_CONFIG_LIST,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "#from TTS/bin/find_unique_chars.py\n",
        "items = train_samples + eval_samples\n",
        "\n",
        "texts = \"\".join(item[\"text\"] for item in items)\n",
        "chars = set(texts)\n",
        "lower_chars = filter(lambda c: c.islower(), chars)\n",
        "chars_force_lower = [c.lower() for c in chars]\n",
        "chars_force_lower = set(chars_force_lower)\n",
        "\n",
        "print(f\" > Number of unique characters: {len(chars)}\")\n",
        "print(f\" > Unique characters: {''.join(sorted(chars))}\")\n",
        "print(f\" > Unique lower characters: {''.join(sorted(lower_chars))}\")\n",
        "print(f\" > Unique all forced to lower characters: {''.join(sorted(chars_force_lower))}\")\n",
        "#https://www.geeksforgeeks.org/python-convert-string-to-unicode-characters/\n",
        "char_str = str(chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gV4P42IQVyTO"
      },
      "outputs": [],
      "source": [
        "print(\"Current reinit_text_encoder value: \" + str(config.model_args.reinit_text_encoder))\n",
        "reinit_te_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if reinit_te_status==\"False\":\n",
        "  print(\"Text encoder will not be reinitilized\")\n",
        "elif reinit_te_status==\"True\":\n",
        "  config.model_args.reinit_text_encoder=True\n",
        "  print(\"Model arguments set to reinitilize text encoder\")\n",
        "  print(\"Current reinit_DP value: \" + str(config.model_args.reinit_DP))\n",
        "reinit_DP_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if reinit_DP_status==\"False\":\n",
        "  print(\"DP will not be reinitilized\")\n",
        "elif reinit_DP_status==\"True\":\n",
        "  config.model_args.reinit_DP=True\n",
        "  print(\"Model arguments set to reinitilize DP\")\n",
        "print(\"Current freeze_waveform_decoder value: \" + str(config.model_args.freeze_waveform_decoder))\n",
        "freeze_waveform_decoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_waveform_decoder_status==\"False\":\n",
        "  print(\"Waveform decoder will NOT be frozen\")\n",
        "  config.model_args.freeze_waveform_decoder=False\n",
        "elif freeze_waveform_decoder_status==\"True\":\n",
        "  config.model_args.freeze_waveform_decoder=True\n",
        "  print(\"Waveform decoder FROZEN\")\n",
        "print(\"Current freeze_flow_decoder value: \" + str(config.model_args.freeze_flow_decoder))\n",
        "freeze_flow_decoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_flow_decoder_status==\"False\":\n",
        "  print(\"Flow decoder will NOT be frozen\")\n",
        "  config.model_args.freeze_flow_decoder=None\n",
        "elif freeze_flow_decoder_status==\"True\":\n",
        "  config.model_args.freeze_flow_decoder=\"True\"\n",
        "  print(\"Flow decoder FROZEN\")\n",
        "print(\"Current freeze_encoder value: \" + str(config.model_args.freeze_encoder))\n",
        "freeze_encoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_encoder_status==\"False\":\n",
        "  print(\"Text encoder will NOT be frozen\")\n",
        "  config.model_args.freeze_encoder=False\n",
        "elif freeze_encoder_status==\"True\":\n",
        "  config.model_args.freeze_encoder=True\n",
        "  print(\"Text encoder FROZEN\")\n",
        "print(\"Current freeze_DP value: \" + str(config.model_args.freeze_DP))\n",
        "freeze_DP_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_DP_status==\"False\":\n",
        "  print(\"Duration predictor will NOT be frozen\")\n",
        "  config.model_args.freeze_DP=False\n",
        "elif freeze_DP_status==\"True\":\n",
        "  config.model_args.freeze_DP=True\n",
        "  print(\"Duration predictor FROZEN\")        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGftc8uHb_IJ"
      },
      "source": [
        "**Initilize the trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Du2F3iajC1"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "print(run_type)\n",
        "\n",
        "if run_type==\"continue\":\n",
        "  CONTINUE_PATH=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder\n",
        "  trainer = Trainer(\n",
        "    TrainerArgs(continue_path=CONTINUE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "    config,\n",
        "    output_path=OUT_PATH,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"restore\":\n",
        "    trainer = Trainer(\n",
        "    TrainerArgs(restore_path=MODEL_FILE, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "    config,\n",
        "    output_path=OUT_PATH,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"restore-ckpt\":\n",
        "  trainer = Trainer(\n",
        "  TrainerArgs(restore_path=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder+\"/\"+ckpt_file, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "  config,\n",
        "  output_path=OUT_PATH,\n",
        "  model=model,\n",
        "  train_samples=train_samples,\n",
        "  eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"newmodel\":\n",
        "  trainer = Trainer(\n",
        "  TrainerArgs(),\n",
        "  config,\n",
        "  output_path=OUT_PATH,\n",
        "  model=model,\n",
        "  train_samples=train_samples,\n",
        "  eval_samples=eval_samples,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GMG6K7c6cF1t"
      },
      "source": [
        "**CHECK THAT TENSORBOARD IS RUNNING ABOVE, THEN RUN THIS TO TRAIN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axmM5IwMabtL"
      },
      "outputs": [],
      "source": [
        "trainer.fit()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SECTION FOR GENERATING SPEECH. NOT CONFIGURED YET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKFmWsV8YJsf"
      },
      "outputs": [],
      "source": [
        "!tts --model_path /content/drive/MyDrive/vits-vctk-multi-ds/traineroutput/vits_vctk-February-09-2023_12+12AM-914280a5/best_model_1003928.pth \\\n",
        "--config_path /content/drive/MyDrive/vits-vctk-multi-ds/traineroutput/vits_vctk-February-09-2023_12+12AM-914280a5/config.json \\\n",
        "--list_speaker_idxs \\\n",
        "--text \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5KEH9Yan-V"
      },
      "outputs": [],
      "source": [
        "out_wav_file =\"/content/drive/MyDrive/me-mmj.wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xQkpjGiZH2U"
      },
      "outputs": [],
      "source": [
        "!tts --model_path /content/drive/MyDrive/vits-vctk-multi-ds/traineroutput/vits_vctk-February-09-2023_12+12AM-914280a5/best_model_1003928.pth \\\n",
        "--config_path /content/drive/MyDrive/vits-vctk-multi-ds/traineroutput/vits_vctk-February-09-2023_12+12AM-914280a5/config.json \\\n",
        "--speaker_idx VCTK_me \\\n",
        "--text \"I am the very model of a modern Major-General,\\\n",
        " I've information vegetable, animal, and mineral, \\\n",
        " I know the kings of England, and I quote the fights historical \\\n",
        " From Marathon to Waterloo, in order categorical; \\\n",
        " I'm very well acquainted, too, with matters mathematical, \\\n",
        " I understand equations, both the simple and quadratical, \\\n",
        "  About binomial theorem I'm teeming with a lot o' news, \\\n",
        "  With many cheerful facts about the square of the hypotenuse.\" \\\n",
        "  --out_path $out_wav_file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56YzZUXuaYjB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from IPython.display import display\n",
        "wn = Audio(out_wav_file, autoplay=False) ##\n",
        "display(wn)##"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
